{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ae5fa7",
   "metadata": {},
   "source": [
    "#### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd86d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from lib.data_handlers import Load_ImageNet100, Load_PACS\n",
    "from overcomplete.models import DinoV2, ViT, ResNet\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from overcomplete.sae import TopKSAE, train_sae\n",
    "from overcomplete.visualization import (overlay_top_heatmaps, evidence_top_images, zoom_top_images, contour_top_image)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "from lib.universal_trainer import train_usae\n",
    "from lib.activation_generator import Load_activation_dataloader\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3417692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7c042",
   "metadata": {},
   "source": [
    "### Train a USAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b998439e",
   "metadata": {},
   "source": [
    "Define Models and Generate Their Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a68d84da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\sproj_ha/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "Processing Batches: 100%|██████████| 508/508 [15:17<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "models = {\n",
    "    \"ViT\": ViT(device=\"cuda\"),\n",
    "    \"DinoV2\": DinoV2(device=\"cuda\")\n",
    "}\n",
    "\n",
    "activation_dir = \"activations/ImageNet100_Dino_ViT\"\n",
    "image_loader = Load_ImageNet100(transform=None, batch_size=256, shuffle=True)\n",
    "\n",
    "\n",
    "activations_dataloader = Load_activation_dataloader(\n",
    "    models=models,\n",
    "    image_dataloader=image_loader,\n",
    "    max_seq_len=196,   \n",
    "    save_dir=activation_dir, \n",
    "    generate=True,  \n",
    "    rearrange_string='n t d -> (n t) d'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35fc11e",
   "metadata": {},
   "source": [
    "SAE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cae4a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = 768 * 8\n",
    "epochs = 20\n",
    "lr=3e-4\n",
    "sample = next(iter(activations_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698beefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   9%|▉         | 48/508 [00:35<03:56,  1.95it/s, loss=1.39]c:\\Users\\sproj_ha\\miniconda3\\envs\\interpretability\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1/20: 100%|██████████| 508/508 [07:01<00:00,  1.21it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1] Loss: 694.6747 | Time: 421.47s | Dead Features: 0.0%\n",
      "ViT Loss: [308.1587703227997]\n",
      "DinoV2 Loss: [386.51594799757004]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 508/508 [13:42<00:00,  1.62s/it, loss=1.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 2] Loss: 676.8938 | Time: 822.66s | Dead Features: 71.3%\n",
      "ViT Loss: [308.1587703227997, 293.40135046839714]\n",
      "DinoV2 Loss: [386.51594799757004, 383.49242997169495]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 508/508 [13:30<00:00,  1.60s/it, loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 3] Loss: 671.6713 | Time: 810.70s | Dead Features: 87.6%\n",
      "ViT Loss: [308.1587703227997, 293.40135046839714, 291.0020546913147]\n",
      "DinoV2 Loss: [386.51594799757004, 383.49242997169495, 380.669264793396]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 508/508 [13:29<00:00,  1.59s/it, loss=1.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 4] Loss: 670.7984 | Time: 809.93s | Dead Features: 90.7%\n",
      "ViT Loss: [308.1587703227997, 293.40135046839714, 291.0020546913147, 290.23292818665504]\n",
      "DinoV2 Loss: [386.51594799757004, 383.49242997169495, 380.669264793396, 380.56543242931366]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 508/508 [13:30<00:00,  1.59s/it, loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 5] Loss: 671.3460 | Time: 810.23s | Dead Features: 91.5%\n",
      "ViT Loss: [308.1587703227997, 293.40135046839714, 291.0020546913147, 290.23292818665504, 289.9391912519932]\n",
      "DinoV2 Loss: [386.51594799757004, 383.49242997169495, 380.669264793396, 380.56543242931366, 381.40680783987045]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 508/508 [13:28<00:00,  1.59s/it, loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 6] Loss: 671.7623 | Time: 808.74s | Dead Features: 90.3%\n",
      "ViT Loss: [308.1587703227997, 293.40135046839714, 291.0020546913147, 290.23292818665504, 289.9391912519932, 289.6635777056217]\n",
      "DinoV2 Loss: [386.51594799757004, 383.49242997169495, 380.669264793396, 380.56543242931366, 381.40680783987045, 382.09873074293137]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 508/508 [13:29<00:00,  1.59s/it, loss=1.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 7] Loss: 672.1710 | Time: 809.28s | Dead Features: 87.7%\n",
      "ViT Loss: [308.1587703227997, 293.40135046839714, 291.0020546913147, 290.23292818665504, 289.9391912519932, 289.6635777056217, 289.7649430334568]\n",
      "DinoV2 Loss: [386.51594799757004, 383.49242997169495, 380.669264793396, 380.56543242931366, 381.40680783987045, 382.09873074293137, 382.4060481786728]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 508/508 [13:32<00:00,  1.60s/it, loss=1.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 8] Loss: 672.6959 | Time: 812.19s | Dead Features: 83.9%\n",
      "ViT Loss: [308.1587703227997, 293.40135046839714, 291.0020546913147, 290.23292818665504, 289.9391912519932, 289.6635777056217, 289.7649430334568, 289.7163984775543]\n",
      "DinoV2 Loss: [386.51594799757004, 383.49242997169495, 380.669264793396, 380.56543242931366, 381.40680783987045, 382.09873074293137, 382.4060481786728, 382.97954457998276]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20:   2%|▏         | 11/508 [00:18<14:07,  1.71s/it, loss=1.4] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     13\u001b[39m   schedulers[key] = SequentialLR(optimizers[key], schedulers=[warmup_scheduler, cosine_scheduler], milestones=[\u001b[32m25\u001b[39m])\n\u001b[32m     16\u001b[39m criterion = nn.L1Loss(reduction=\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# change to mean reduction \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m train_usae(names=\u001b[38;5;28mlist\u001b[39m(models.keys()),\n\u001b[32m     19\u001b[39m            models=SAEs,\n\u001b[32m     20\u001b[39m            dataloader=activations_dataloader,\n\u001b[32m     21\u001b[39m            criterion=criterion,\n\u001b[32m     22\u001b[39m            nb_epochs=epochs,\n\u001b[32m     23\u001b[39m            optimizers=optimizers,\n\u001b[32m     24\u001b[39m            schedulers=schedulers,\n\u001b[32m     25\u001b[39m            device=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sproj_ha\\Desktop\\vision_interp\\lib\\universal_trainer.py:112\u001b[39m, in \u001b[36mtrain_usae\u001b[39m\u001b[34m(names, models, dataloader, criterion, optimizers, schedulers, nb_epochs, clip_grad, monitoring, device)\u001b[39m\n\u001b[32m    109\u001b[39m dead_tracker.update(z)\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Update metrics and tqdm bar\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m batch_loss = total_loss.item()\n\u001b[32m    113\u001b[39m epoch_loss += batch_loss\n\u001b[32m    114\u001b[39m progress_bar.set_postfix(loss=batch_loss)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "SAEs = {}\n",
    "optimizers = {}\n",
    "schedulers = {}\n",
    "\n",
    "for key, model in models.items():\n",
    "  \n",
    "  SAEs[key] = TopKSAE(sample[f\"activations_{key}\"].shape[-1], nb_concepts=concepts, top_k=32, device='cuda')\n",
    "  optimizers[key] = torch.optim.Adam(SAEs[key].parameters(), lr=lr)\n",
    "\n",
    "  # Set up a Linear + Cosine Scheduler\n",
    "  warmup_scheduler = LinearLR(optimizers[key], start_factor=1e-6 / 3e-4, end_factor=1.0, total_iters=25)\n",
    "  cosine_scheduler = CosineAnnealingLR(optimizers[key], T_max=epochs, eta_min=1e-6)\n",
    "  schedulers[key] = SequentialLR(optimizers[key], schedulers=[warmup_scheduler, cosine_scheduler], milestones=[25])\n",
    "\n",
    "\n",
    "criterion = nn.L1Loss(reduction=\"mean\") # change to mean reduction \n",
    "\n",
    "train_usae(names=list(models.keys()),\n",
    "           models=SAEs,\n",
    "           dataloader=activations_dataloader,\n",
    "           criterion=criterion,\n",
    "           nb_epochs=epochs,\n",
    "           optimizers=optimizers,\n",
    "           schedulers=schedulers,\n",
    "           device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dicts = {name: model.state_dict() for name, model in SAEs.items()}\n",
    "torch.save(model_state_dicts, \"./models/USAEs_DinoViT_V5_200ep_fullconcepts.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f35e63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the funny part, we have access to 4 functions that allow us to inspect the concepts,\n",
    "# let use them to understand a bit more the top 3 concepts !\n",
    "\n",
    "\n",
    "## Create a tensor to save a list of top activations\n",
    "topk = int(0.08 * 768 * 7) \n",
    "selected_concepts = torch.zeros(topk+1)\n",
    "activations = next(iter(activations_dataloader))\n",
    "\n",
    "for i, (key, model) in enumerate(models.items()):\n",
    "  sae = SAEs[key]\n",
    "  Activations = activations[f'activations_{key}'].to(device)\n",
    "  with torch.no_grad():\n",
    "    pre_codes, codes = sae.encode(Activations.squeeze())\n",
    "    \n",
    "    codes = rearrange(codes, '(n w h) d -> n w h d', w=16, h=16)\n",
    "    \n",
    "    codes_flat = codes.abs().sum(dim=(1, 2))        \n",
    "    concept_strength = codes_flat.sum(dim=0)        \n",
    "    top_concepts = torch.argsort(concept_strength, descending=True)[:topk].to(device)\n",
    "    selected_concepts[i:i + topk] = top_concepts\n",
    "\n",
    "\n",
    "\n",
    "# Overlay Top 20 for this model\n",
    "for id in selected_concepts:\n",
    "#for concept_id in range(50):\n",
    "  concept_id = int(id.item())\n",
    "  for key, model in models.items():\n",
    "    sae = SAEs[key]\n",
    "    Activations = activations[f'activations_{key}'].to(device)\n",
    "    with torch.no_grad():\n",
    "      pre_codes, codes = sae.encode(Activations.squeeze())\n",
    "\n",
    "    codes = rearrange(codes, '(n w h) d -> n w h d', w=14, h=14)\n",
    "    \n",
    "    save_dir = f\"results/usae_run8/{key}_concepts\"\n",
    "\n",
    "    overlay_top_heatmaps(activations[f\"images\"].squeeze(), codes, concept_id=concept_id)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filename = f\"concept_{concept_id}_{key}.png\"\n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    plt.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb42d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interpretability",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
