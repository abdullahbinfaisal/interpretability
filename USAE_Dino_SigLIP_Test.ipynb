{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ae5fa7",
   "metadata": {},
   "source": [
    "#### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd86d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from lib.data_handlers import Load_ImageNet100, Load_PACS\n",
    "from overcomplete.models import DinoV2, ViT, ResNet, ViT_Large, SigLIP\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from overcomplete.sae import TopKSAE, train_sae\n",
    "from overcomplete.visualization import (overlay_top_heatmaps, evidence_top_images, zoom_top_images, contour_top_image)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "from lib.universal_trainer import train_usae\n",
    "from lib.activation_generator import Load_activation_dataloader\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "from lib.eval import evaluate_models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7c042",
   "metadata": {},
   "source": [
    "### Train a USAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b998439e",
   "metadata": {},
   "source": [
    "Define Models and Generate Their Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a68d84da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sproj_ha\\miniconda3\\envs\\interpretability\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"ViT\": ViT(device=\"cuda\"),\n",
    "    \"SigLIP\": SigLIP(device=\"cuda\")\n",
    "}\n",
    "\n",
    "activation_dir = \"activations/ImageNet100_ViT_SigLIP\"\n",
    "image_loader = Load_ImageNet100(transform=None, batch_size=256, shuffle=True)\n",
    "\n",
    "\n",
    "activations_dataloader = Load_activation_dataloader(\n",
    "    models=models,\n",
    "    image_dataloader=image_loader,\n",
    "    max_seq_len=196,   \n",
    "    save_dir=activation_dir, \n",
    "    generate=False,  \n",
    "    rearrange_string='n t d -> (n t) d'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba5114a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ViT →  7.98%\n",
      "    SigLIP →  0.00%\n"
     ]
    }
   ],
   "source": [
    "val_path = r\"C:\\Users\\sproj_ha\\Desktop\\vision_interp\\datasets\\imagenet100\\val.X\"\n",
    "accs = evaluate_models(models, val_path, batch_size=32)\n",
    "for name, a in accs.items():\n",
    "    print(f\"{name:>10s} → {a*100:5.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35fc11e",
   "metadata": {},
   "source": [
    "SAE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cae4a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = 768 * 8\n",
    "epochs = 5\n",
    "lr=3e-4\n",
    "sample = next(iter(activations_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "698beefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   9%|▉         | 48/508 [00:19<02:50,  2.71it/s, loss=1.6] c:\\Users\\sproj_ha\\miniconda3\\envs\\interpretability\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1/5:  17%|█▋        | 86/508 [00:34<02:51,  2.47it/s, loss=1.52]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     20\u001b[39m     schedulers[key] = SequentialLR(\n\u001b[32m     21\u001b[39m         optimizers[key],\n\u001b[32m     22\u001b[39m         schedulers=[warmup_scheduler, cosine_scheduler],\n\u001b[32m     23\u001b[39m         milestones=[\u001b[32m25\u001b[39m],\n\u001b[32m     24\u001b[39m     )\n\u001b[32m     26\u001b[39m criterion = nn.L1Loss(reduction=\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# change to mean reduction\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m train_usae(\n\u001b[32m     29\u001b[39m     names=\u001b[38;5;28mlist\u001b[39m(models.keys()),\n\u001b[32m     30\u001b[39m     models=SAEs,\n\u001b[32m     31\u001b[39m     dataloader=activations_dataloader,\n\u001b[32m     32\u001b[39m     criterion=criterion,\n\u001b[32m     33\u001b[39m     nb_epochs=epochs,\n\u001b[32m     34\u001b[39m     optimizers=optimizers,\n\u001b[32m     35\u001b[39m     schedulers=schedulers,\n\u001b[32m     36\u001b[39m     device=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     37\u001b[39m     seed=\u001b[32m42\u001b[39m,\n\u001b[32m     38\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sproj_ha\\Desktop\\vision_interp\\lib\\universal_trainer.py:95\u001b[39m, in \u001b[36mtrain_usae\u001b[39m\u001b[34m(names, models, dataloader, criterion, optimizers, schedulers, nb_epochs, clip_grad, device, seed)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     93\u001b[39m         x_hat = m.decode(z.detach())\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m target = batch[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mactivations_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m].squeeze().to(device)\n\u001b[32m     96\u001b[39m loss = criterion(x_hat, target)\n\u001b[32m     98\u001b[39m epoch_model_losses[n] += loss.item()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "SAEs = {}\n",
    "optimizers = {}\n",
    "schedulers = {}\n",
    "\n",
    "for key, model in models.items():\n",
    "\n",
    "    SAEs[key] = TopKSAE(\n",
    "        sample[f\"activations_{key}\"].shape[-1],\n",
    "        nb_concepts=concepts,\n",
    "        top_k=64,\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "    optimizers[key] = torch.optim.Adam(SAEs[key].parameters(), lr=lr)\n",
    "\n",
    "    # Set up a Linear + Cosine Scheduler\n",
    "    warmup_scheduler = LinearLR(\n",
    "        optimizers[key], start_factor=1e-6 / 3e-4, end_factor=1.0, total_iters=25\n",
    "    )\n",
    "    cosine_scheduler = CosineAnnealingLR(optimizers[key], T_max=epochs, eta_min=1e-6)\n",
    "    schedulers[key] = SequentialLR(\n",
    "        optimizers[key],\n",
    "        schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "        milestones=[25],\n",
    "    )\n",
    "\n",
    "criterion = nn.L1Loss(reduction=\"mean\")  # change to mean reduction\n",
    "\n",
    "train_usae(\n",
    "    names=list(models.keys()),\n",
    "    models=SAEs,\n",
    "    dataloader=activations_dataloader,\n",
    "    criterion=criterion,\n",
    "    nb_epochs=epochs,\n",
    "    optimizers=optimizers,\n",
    "    schedulers=schedulers,\n",
    "    device=\"cuda\",\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dicts = {name: model.state_dict() for name, model in SAEs.items()}\n",
    "torch.save(model_state_dicts, \"./models/USAE_ViT_SigLIP_20epoch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the funny part, we have access to 4 functions that allow us to inspect the concepts,\n",
    "# let use them to understand a bit more the top 3 concepts !\n",
    "\n",
    "\n",
    "## Create a tensor to save a list of top activations\n",
    "topk = int(0.08 * 768 * 8) \n",
    "selected_concepts = torch.zeros(topk+1)\n",
    "activations = next(iter(activations_dataloader))\n",
    "\n",
    "for i, (key, model) in enumerate(models.items()):\n",
    "  sae = SAEs[key]\n",
    "  Activations = activations[f'activations_{key}'].to(device)\n",
    "  with torch.no_grad():\n",
    "    pre_codes, codes = sae.encode(Activations.squeeze())\n",
    "    \n",
    "    codes = rearrange(codes, '(n w h) d -> n w h d', w=16, h=16)\n",
    "    \n",
    "    codes_flat = codes.abs().sum(dim=(1, 2))        \n",
    "    concept_strength = codes_flat.sum(dim=0)        \n",
    "    top_concepts = torch.argsort(concept_strength, descending=True)[:topk].to(device)\n",
    "    selected_concepts[i:i + topk] = top_concepts\n",
    "\n",
    "\n",
    "\n",
    "# Overlay Top 20 for this model\n",
    "for id in selected_concepts:\n",
    "#for concept_id in range(50):\n",
    "  concept_id = int(id.item())\n",
    "  for key, model in models.items():\n",
    "    sae = SAEs[key]\n",
    "    Activations = activations[f'activations_{key}'].to(device)\n",
    "    with torch.no_grad():\n",
    "      pre_codes, codes = sae.encode(Activations.squeeze())\n",
    "\n",
    "    codes = rearrange(codes, '(n w h) d -> n w h d', w=14, h=14)\n",
    "    \n",
    "    save_dir = f\"results/usae_run10_vit_siglip/{key}_concepts\"\n",
    "\n",
    "    overlay_top_heatmaps(activations[f\"images\"].squeeze(), codes, concept_id=concept_id)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filename = f\"concept_{concept_id}_{key}.png\"\n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    plt.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb42d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interpretability",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
